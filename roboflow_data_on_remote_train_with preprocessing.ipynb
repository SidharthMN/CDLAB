{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SidharthMN/CDLAB/blob/main/roboflow_data_on_remote_train_with%20preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###install drivers"
      ],
      "metadata": {
        "id": "P-_JjKfuwPEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnQeiht9vvmw",
        "outputId": "7bb15310-4925-4a32-a3c9-c364cfc09f32"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Feb 18 18:08:06 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luENE64ITLqd"
      },
      "source": [
        "###install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyOJC0LTxvOR",
        "outputId": "23c3f074-9d6e-4df9-b2ac-4ea1c9bb55e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.4.14-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.13.0.92)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu128)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu128)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (26.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2026.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.21.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.4.14-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.4.14 ultralytics-thop-2.0.18\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5ooERhdTSzM"
      },
      "source": [
        "###Install roboflow data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpXfWtpHx4xT",
        "outputId": "3a99dde1-3f56-4065-df30-13d9092f264b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.2.14-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2026.1.4)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (11.3.0)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow)\n",
            "  Downloading pillow_avif_plugin-1.5.5-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.67.3)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (6.0.3)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting pi-heif<2 (from roboflow)\n",
            "  Downloading pi_heif-1.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (4.61.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (26.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (3.3.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow) (3.4.4)\n",
            "Downloading roboflow-1.2.14-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.5-cp312-cp312-manylinux_2_28_x86_64.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m147.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.13.0.92\n",
            "    Uninstalling opencv-python-headless-4.13.0.92:\n",
            "      Successfully uninstalled opencv-python-headless-4.13.0.92\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.2.1 pillow-avif-plugin-1.5.5 roboflow-1.2.14\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Red-light-violation-2 to yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 693907/693907 [00:37<00:00, 18561.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Red-light-violation-2 in yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23516/23516 [00:04<00:00, 5497.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"pSHla59oUMBHPukNOML1\")\n",
        "project = rf.workspace(\"sidsoid\").project(\"red-light-violation-xsiyy-pz7co\")\n",
        "version = project.version(2)\n",
        "dataset = version.download(\"yolov8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing Steps"
      ],
      "metadata": {
        "id": "jLE55r7YtNxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Remove null images"
      ],
      "metadata": {
        "id": "6RbFxHn35Pfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "# ğŸ” CHANGE THIS PATH to your dataset images folder\n",
        "DATASET_PATH = \"/content/dataset/images\"  # e.g. /content/train/images\n",
        "\n",
        "removed = 0\n",
        "valid = 0\n",
        "\n",
        "for root, dirs, files in os.walk(DATASET_PATH):\n",
        "    for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "\n",
        "        # Check if file is an image (jpg, png, jpeg)\n",
        "        if not file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.webp')):\n",
        "            print(f\"Removing non-image file: {file}\")\n",
        "            os.remove(file_path)\n",
        "            removed += 1\n",
        "            continue\n",
        "\n",
        "        # Check file size (0 KB images)\n",
        "        if os.path.getsize(file_path) == 0:\n",
        "            print(f\"Removing empty image: {file}\")\n",
        "            os.remove(file_path)\n",
        "            removed += 1\n",
        "            continue\n",
        "\n",
        "        # Try reading the image\n",
        "        img = cv2.imread(file_path)\n",
        "\n",
        "        # If image is None (corrupt/null)\n",
        "        if img is None:\n",
        "            print(f\"Removing corrupt image: {file}\")\n",
        "            os.remove(file_path)\n",
        "            removed += 1\n",
        "            continue\n",
        "\n",
        "        # Check invalid dimensions\n",
        "        h, w = img.shape[:2]\n",
        "        if h == 0 or w == 0:\n",
        "            print(f\"Removing invalid dimension image: {file}\")\n",
        "            os.remove(file_path)\n",
        "            removed += 1\n",
        "        else:\n",
        "            valid += 1\n",
        "\n",
        "print(\"\\nâœ… Preprocessing Complete!\")\n",
        "print(f\"Valid Images: {valid}\")\n",
        "print(f\"Removed (Null/Corrupt) Images: {removed}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3rRRzQ6tJrJ",
        "outputId": "171f8905-c8c4-425e-e85e-83a6501243b0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Preprocessing Complete!\n",
            "Valid Images: 0\n",
            "Removed (Null/Corrupt) Images: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Brightness"
      ],
      "metadata": {
        "id": "8vehviCP5jHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# ğŸ” CHANGE THESE PATHS\n",
        "input_folder = os.path.join(dataset.location, \"train\", \"images\") # Changed to use dataset.location\n",
        "output_folder = \"/content/dataset_bright/images\"\n",
        "\n",
        "# Create output folder\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Brightness factor\n",
        "# >1.0 = brighter\n",
        "# <1.0 = darker\n",
        "brightness_factor = 1.3  # try 1.2 to 1.5 for low-light datasets\n",
        "\n",
        "processed = 0\n",
        "\n",
        "for file in os.listdir(input_folder):\n",
        "    input_path = os.path.join(input_folder, file)\n",
        "\n",
        "    if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.webp')):\n",
        "        img = cv2.imread(input_path)\n",
        "\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        # Convert to float for brightness scaling\n",
        "        bright_img = cv2.convertScaleAbs(img, alpha=brightness_factor, beta=0)\n",
        "\n",
        "        # Save image\n",
        "        output_path = os.path.join(output_folder, file)\n",
        "        cv2.imwrite(output_path, bright_img)\n",
        "        processed += 1\n",
        "\n",
        "print(f\"âœ… Brightness preprocessing complete! Processed images: {processed}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQUFTuDjtaSg",
        "outputId": "97de6db5-8b60-4e83-ff53-edf7d9b0e24a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Brightness preprocessing complete! Processed images: 10239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Motion Blur"
      ],
      "metadata": {
        "id": "HPEpuZqy5v0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ğŸ” CHANGE PATHS\n",
        "input_folder = os.path.join(dataset.location, 'train', 'images') # Corrected path\n",
        "output_folder = \"/content/dataset_motion_blur/images\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Motion blur strength (0.2 = light blur)\n",
        "blur_strength = 0.2\n",
        "\n",
        "def apply_motion_blur(image, strength=0.2):\n",
        "    h, w = image.shape[:2]\n",
        "\n",
        "    # Kernel size based on strength (adaptive)\n",
        "    ksize = max(3, int(min(h, w) * strength * 0.05))\n",
        "\n",
        "    # Ensure kernel size is odd\n",
        "    if ksize % 2 == 0:\n",
        "        ksize += 1\n",
        "\n",
        "    # Create horizontal motion blur kernel\n",
        "    kernel = np.zeros((ksize, ksize))\n",
        "    kernel[int((ksize - 1) / 2), :] = np.ones(ksize)\n",
        "    kernel = kernel / ksize\n",
        "\n",
        "    # Apply filter\n",
        "    blurred = cv2.filter2D(image, -1, kernel)\n",
        "    return blurred\n",
        "\n",
        "processed = 0\n",
        "\n",
        "for file in os.listdir(input_folder):\n",
        "    if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.webp')):\n",
        "        img_path = os.path.join(input_folder, file)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        # Apply motion blur (0.2)\n",
        "        motion_blurred = apply_motion_blur(img, blur_strength)\n",
        "\n",
        "        # Save output\n",
        "        output_path = os.path.join(output_folder, file)\n",
        "        cv2.imwrite(output_path, motion_blurred)\n",
        "        processed += 1\n",
        "\n",
        "print(f\"ğŸ¯ Motion Blur (0.2) applied to {processed} images!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p418v8G8ttMy",
        "outputId": "1265a583-ef40-406b-8942-f62e84f19448"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¯ Motion Blur (0.2) applied to 10239 images!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Blur"
      ],
      "metadata": {
        "id": "e0sYK_F654G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# âŒ Change paths\n",
        "input_folder = os.path.join(dataset.location, 'train', 'images') # Corrected path\n",
        "output_folder = \"/content/dataset_blur_1px/images\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "processed = 0\n",
        "\n",
        "for file in os.listdir(input_folder):\n",
        "    if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.webp')):\n",
        "        img_path = os.path.join(input_folder, file)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        # 1px blur (very light blur)\n",
        "        blurred = cv2.GaussianBlur(img, (3, 3), 0)\n",
        "\n",
        "        # Save image\n",
        "        cv2.imwrite(os.path.join(output_folder, file), blurred)\n",
        "        processed += 1\n",
        "\n",
        "print(f\"âœ… 1px Blur applied to {processed} images!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYT_B5K6t5zk",
        "outputId": "5fa927ec-57e8-4f71-8b24-60c7671dfab2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… 1px Blur applied to 10239 images!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7cjp5zcTkIk"
      },
      "source": [
        "###Train YOLO (takes time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJFiaA4Ax-pS",
        "outputId": "85114973-2f95-4a28-9eef-e91fc975bda1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolov8n.pt to 'yolov8n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.2MB 116.4MB/s 0.1s\n",
            "Ultralytics 8.4.14 ğŸš€ Python-3.12.12 torch-2.9.0+cu128 CUDA:0 (Tesla T4, 14913MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/Bike-Helmet-Detection-1/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=512, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 25.2MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, 16, None, [64, 128, 256]] \n",
            "Model summary: 130 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n.pt to 'yolo26n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.3MB 104.6MB/s 0.1s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 719.2Â±362.3 MB/s, size: 34.4 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/Bike-Helmet-Detection-1/train/labels... 3534 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3534/3534 2.3Kit/s 1.6s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/Bike-Helmet-Detection-1/train/labels.cache\n",
            "WARNING âš ï¸ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.6GB RAM): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3534/3534 520.2it/s 6.8s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 979.2Â±824.7 MB/s, size: 34.9 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Bike-Helmet-Detection-1/valid/labels... 127 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 127/127 1.6Kit/s 0.1s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Bike-Helmet-Detection-1/valid/labels.cache\n",
            "WARNING âš ï¸ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB RAM): 100% â”â”â”â”â”â”â”â”â”â”â”â” 127/127 257.9it/s 0.5s\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Plotting labels to /content/runs/detect/train/labels.jpg... \n",
            "Image sizes 512 train, 512 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/train\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/30     0.736G      1.597      1.932      1.278         21        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 7.7it/s 57.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 4.4it/s 1.8s\n",
            "                   all        127        300      0.569      0.648      0.608      0.304\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/30     0.883G      1.477       1.34       1.23         16        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.3it/s 53.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 11.4it/s 0.7s\n",
            "                   all        127        300      0.682      0.627      0.666      0.332\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/30       0.9G      1.441      1.186      1.213         24        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.8it/s 50.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 8.3it/s 1.0s\n",
            "                   all        127        300      0.655      0.684      0.696       0.33\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/30     0.918G      1.408      1.095      1.201         11        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.6it/s 51.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 11.8it/s 0.7s\n",
            "                   all        127        300      0.752      0.616      0.703      0.363\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/30     0.934G      1.356      1.019      1.173         18        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.5it/s 52.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 12.0it/s 0.7s\n",
            "                   all        127        300      0.671      0.642      0.707      0.357\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/30     0.951G      1.318     0.9385       1.15          9        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.8it/s 50.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 8.4it/s 1.0s\n",
            "                   all        127        300      0.674      0.687      0.689      0.332\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/30     0.969G      1.289     0.9009      1.139         30        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.1it/s 54.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 11.7it/s 0.7s\n",
            "                   all        127        300      0.654      0.695      0.687      0.342\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/30     0.986G      1.278     0.8521      1.132         27        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.4it/s 52.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 12.4it/s 0.6s\n",
            "                   all        127        300      0.697      0.666      0.696       0.37\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/30         1G      1.243     0.8339      1.113         13        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.2it/s 54.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 12.1it/s 0.7s\n",
            "                   all        127        300      0.704      0.697      0.705      0.371\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/30      1.02G      1.213     0.7979      1.103         38        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.0it/s 55.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 11.2it/s 0.7s\n",
            "                   all        127        300      0.833      0.664      0.761      0.402\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/30      1.04G      1.198     0.7734      1.095         26        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.2it/s 54.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 12.0it/s 0.7s\n",
            "                   all        127        300      0.677      0.691      0.727      0.364\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/30      1.05G      1.172     0.7426       1.08         25        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.5it/s 51.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 8.0it/s 1.0s\n",
            "                   all        127        300      0.705      0.687      0.742      0.388\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/30      1.07G      1.156     0.7094      1.071         32        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.3it/s 53.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 11.5it/s 0.7s\n",
            "                   all        127        300      0.752      0.642      0.722      0.368\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/30      1.09G      1.126     0.7032      1.063         24        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.2it/s 54.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 12.0it/s 0.7s\n",
            "                   all        127        300      0.759      0.721      0.758      0.404\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/30      1.11G      1.115     0.6906      1.051         11        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.1it/s 54.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 11.5it/s 0.7s\n",
            "                   all        127        300      0.779      0.678      0.753      0.406\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/30      1.12G      1.099     0.6592      1.043         27        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.0it/s 55.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 11.4it/s 0.7s\n",
            "                   all        127        300      0.707      0.737      0.748      0.396\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/30      1.14G      1.084     0.6574      1.044         27        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.2it/s 54.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 7.2it/s 1.1s\n",
            "                   all        127        300      0.772      0.736      0.779      0.396\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/30      1.16G      1.069     0.6318      1.034         22        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.0it/s 55.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 7.0it/s 1.1s\n",
            "                   all        127        300      0.756      0.723       0.74      0.392\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/30      1.17G      1.057     0.6235      1.029         19        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.2it/s 54.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 9.7it/s 0.8s\n",
            "                   all        127        300       0.78      0.691       0.76      0.389\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/30      1.19G      1.044     0.6111       1.02         21        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.2it/s 54.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 11.9it/s 0.7s\n",
            "                   all        127        300      0.735      0.698      0.735      0.387\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/30      1.21G      1.001     0.5335      1.019         19        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.3it/s 53.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 12.2it/s 0.7s\n",
            "                   all        127        300      0.732      0.685      0.754      0.393\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/30      1.22G     0.9913     0.5211      1.019         38        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.5it/s 51.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 8.4it/s 0.9s\n",
            "                   all        127        300      0.698      0.711       0.72      0.376\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/30      1.24G     0.9613     0.5008      1.002         12        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.8it/s 50.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 11.4it/s 0.7s\n",
            "                   all        127        300      0.722      0.706       0.73      0.379\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/30      1.26G     0.9271     0.4764      0.982          9        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.5it/s 51.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 11.5it/s 0.7s\n",
            "                   all        127        300      0.761      0.644      0.726      0.393\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/30      1.28G     0.9194     0.4695     0.9817         21        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.6it/s 51.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 8.3it/s 1.0s\n",
            "                   all        127        300      0.728      0.731      0.736      0.392\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/30      1.29G     0.9094     0.4641      0.983          8        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.8it/s 50.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 11.8it/s 0.7s\n",
            "                   all        127        300      0.771      0.707      0.742      0.389\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/30      1.31G      0.869     0.4358     0.9616         19        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.7it/s 51.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 12.1it/s 0.7s\n",
            "                   all        127        300      0.705      0.703      0.733      0.396\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/30      1.33G       0.86     0.4394     0.9592         22        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.9it/s 49.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 10.8it/s 0.7s\n",
            "                   all        127        300      0.698      0.732       0.73      0.384\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/30      1.34G     0.8403     0.4251     0.9512         20        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 8.8it/s 50.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 11.6it/s 0.7s\n",
            "                   all        127        300      0.784      0.668      0.736      0.399\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/30      1.36G     0.8201      0.409     0.9427         18        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 442/442 9.0it/s 49.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 8.2it/s 1.0s\n",
            "                   all        127        300      0.707      0.733      0.747      0.401\n",
            "\n",
            "30 epochs completed in 0.448 hours.\n",
            "Optimizer stripped from /content/runs/detect/train/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/runs/detect/train/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/runs/detect/train/weights/best.pt...\n",
            "Ultralytics 8.4.14 ğŸš€ Python-3.12.12 torch-2.9.0+cu128 CUDA:0 (Tesla T4, 14913MiB)\n",
            "Model summary (fused): 73 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 3.1it/s 2.6s\n",
            "                   all        127        300       0.78      0.678      0.753      0.406\n",
            "           With Helmet         89        185      0.822      0.851      0.892      0.507\n",
            "        Without Helmet         53        115      0.738      0.504      0.614      0.304\n",
            "Speed: 0.8ms preprocess, 5.4ms inference, 0.0ms loss, 5.1ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/train\u001b[0m\n",
            "ğŸ’¡ Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ],
      "source": [
        "!yolo task=detect mode=train \\\n",
        "  model=yolov8n.pt \\\n",
        "  data={dataset.location}/data.yaml \\\n",
        "  epochs=30 \\\n",
        "  imgsz=512 \\\n",
        "  batch=8 \\\n",
        "  device=0 \\\n",
        "  cache=True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ded7819"
      },
      "source": [
        "###Test YOLO Model\n",
        "\n",
        "Now that you have trained your YOLO model, you can use it to make predictions. The following code will load your trained model and run inference on an image. The results will be saved in the `runs/detect/predict` directory."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the custom trained model\n",
        "model = YOLO('runs/detect/train/weights/best.pt')  # Ensure this path is correct\n",
        "\n",
        "# Run inference on an image\n",
        "# You can replace 'https://----' with your own image path or a URL\n",
        "results = model.predict(source='https://static.toiimg.com/thumb/msid-94412660,width-1070,height-580,imgsize-427383,resizemode-75,overlay-toi_sw,pt-32,y_pad-40/photo.jpg', save=True, conf=0.25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYmwk1UOyr6c",
        "outputId": "e651352c-aacd-40ae-ea76-ab17072c54dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "\u001b[KDownloading https://static.toiimg.com/thumb/msid-94412660,width-1070,height-580,imgsize-427383,resizemode-75,overlay-toi_sw,pt-32,y_pad-40/photo.jpg to 'photo.jpg': 14% â”â•¸â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 152.0KB/1.0MB 213.4KB/s 0.2s<4.3sWARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "\u001b[KDownloading https://static.toiimg.com/thumb/msid-94412660,width-1070,height-580,imgsize-427383,resizemode-75,overlay-toi_sw,pt-32,y_pad-40/photo.jpg to 'photo.jpg': 30% â”â”â”â•¸â”€â”€â”€â”€â”€â”€â”€â”€ 320.0KB/1.0MB 1.6MB/s 0.3s<0.5sWARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "\u001b[KDownloading https://static.toiimg.com/thumb/msid-94412660,width-1070,height-580,imgsize-427383,resizemode-75,overlay-toi_sw,pt-32,y_pad-40/photo.jpg to 'photo.jpg': 41% â”â”â”â”â•¸â”€â”€â”€â”€â”€â”€â”€ 432.0KB/1.0MB 439.1KB/s 0.4s<1.4sWARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "\u001b[KDownloading https://static.toiimg.com/thumb/msid-94412660,width-1070,height-580,imgsize-427383,resizemode-75,overlay-toi_sw,pt-32,y_pad-40/photo.jpg to 'photo.jpg': 61% â”â”â”â”â”â”â”â”€â”€â”€â”€â”€ 648.0KB/1.0MB 2.1MB/s 0.5s<0.2sWARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "\u001b[KDownloading https://static.toiimg.com/thumb/msid-94412660,width-1070,height-580,imgsize-427383,resizemode-75,overlay-toi_sw,pt-32,y_pad-40/photo.jpg to 'photo.jpg': 84% â”â”â”â”â”â”â”â”â”â”â”€â”€ 896.0KB/1.0MB 2.3MB/s 0.6s<0.1sWARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "\u001b[KDownloading https://static.toiimg.com/thumb/msid-94412660,width-1070,height-580,imgsize-427383,resizemode-75,overlay-toi_sw,pt-32,y_pad-40/photo.jpg to 'photo.jpg': 100% â”â”â”â”â”â”â”â”â”â”â”â” 1.0MB 1.5MB/s 0.7s\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "image 1/1 /content/photo.jpg: 288x512 1 With Helmet, 1 Without Helmet, 8.8ms\n",
            "Speed: 2.3ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 512)\n",
            "Results saved to \u001b[1m/content/runs/detect/predict9\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pF_fZbwTxhd"
      },
      "source": [
        "# download model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iERt0JIByBvb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "b13efe7b-6467-4feb-bd5c-b1c4857a292a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed best.pt to helmet.pt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b6460127-2dcf-47dc-876d-0d9e176e523d\", \"helmet.pt\", 6227946)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n",
            "WARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if os.path.exists(\"runs/detect/train/weights/best.pt\"):\n",
        "  os.rename(\"runs/detect/train/weights/best.pt\", \"runs/detect/train/weights/helmet.pt\")\n",
        "  print(\"Renamed best.pt to helmet.pt\")\n",
        "else:\n",
        "  print(\"best.pt not found. Please ensure the training completed successfully.\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"runs/detect/train/weights/helmet.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b52f7c1"
      },
      "source": [
        "# Task\n",
        "Create a combined dataset for YOLO training by organizing original and augmented image and label files into new training, validation, and test directories, generating a new `data.yaml` file to reflect these changes, and then training a YOLO model using this consolidated dataset. Finally, summarize the data combining process and the outcomes of the YOLO model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09831f2d"
      },
      "source": [
        "## Create Combined Dataset Directories\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c556953a",
        "outputId": "c80a180a-096b-492a-b586-6fcd965f5241"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the root directory for the combined dataset\n",
        "combined_dataset_path = \"/content/combined_dataset\"\n",
        "\n",
        "# Create the main combined dataset directory\n",
        "os.makedirs(combined_dataset_path, exist_ok=True)\n",
        "print(f\"Created directory: {combined_dataset_path}\")\n",
        "\n",
        "# Define the splits and subfolders\n",
        "splits = [\"train\", \"valid\", \"test\"]\n",
        "subfolders = [\"images\", \"labels\"]\n",
        "\n",
        "# Create subdirectories for each split (train, valid, test) and their respective images/labels folders\n",
        "for split in splits:\n",
        "    for subfolder in subfolders:\n",
        "        path = os.path.join(combined_dataset_path, split, subfolder)\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        print(f\"Created directory: {path}\")\n",
        "\n",
        "print(\"\\nâœ… Combined dataset directory structure created successfully!\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: /content/combined_dataset\n",
            "Created directory: /content/combined_dataset/train/images\n",
            "Created directory: /content/combined_dataset/train/labels\n",
            "Created directory: /content/combined_dataset/valid/images\n",
            "Created directory: /content/combined_dataset/valid/labels\n",
            "Created directory: /content/combined_dataset/test/images\n",
            "Created directory: /content/combined_dataset/test/labels\n",
            "\n",
            "âœ… Combined dataset directory structure created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f596c029"
      },
      "source": [
        "## Copy Original Dataset Files\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b664f09f",
        "outputId": "c55d47bb-252d-4594-dfc8-d735369046b1"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define the base paths\n",
        "original_dataset_base_path = dataset.location # This comes from the roboflow download\n",
        "combined_dataset_base_path = \"/content/combined_dataset\"\n",
        "\n",
        "# Splits to process\n",
        "splits = [\"train\", \"valid\", \"test\"]\n",
        "\n",
        "for split in splits:\n",
        "    # Define source paths for images and labels\n",
        "    source_images_path = os.path.join(original_dataset_base_path, split, \"images\")\n",
        "    source_labels_path = os.path.join(original_dataset_base_path, split, \"labels\")\n",
        "\n",
        "    # Define destination paths in the combined dataset\n",
        "    dest_images_path = os.path.join(combined_dataset_base_path, split, \"images\")\n",
        "    dest_labels_path = os.path.join(combined_dataset_base_path, split, \"labels\")\n",
        "\n",
        "    print(f\"\\nCopying original {split} data...\")\n",
        "\n",
        "    # Copy images\n",
        "    if os.path.exists(source_images_path):\n",
        "        for file_name in os.listdir(source_images_path):\n",
        "            if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.webp')):\n",
        "                shutil.copy(os.path.join(source_images_path, file_name), dest_images_path)\n",
        "        print(f\"  Copied {len([f for f in os.listdir(source_images_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.webp'))])} images to {dest_images_path}\")\n",
        "    else:\n",
        "        print(f\"  Source images path not found: {source_images_path}\")\n",
        "\n",
        "    # Copy labels\n",
        "    if os.path.exists(source_labels_path):\n",
        "        for file_name in os.listdir(source_labels_path):\n",
        "            if file_name.lower().endswith(('.txt')):\n",
        "                shutil.copy(os.path.join(source_labels_path, file_name), dest_labels_path)\n",
        "        print(f\"  Copied {len([f for f in os.listdir(source_labels_path) if f.lower().endswith(('.txt'))])} labels to {dest_labels_path}\")\n",
        "    else:\n",
        "        print(f\"  Source labels path not found: {source_labels_path}\")\n",
        "\n",
        "print(\"\\nâœ… Original dataset files copied successfully to combined dataset!\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Copying original train data...\n",
            "  Copied 10239 images to /content/combined_dataset/train/images\n",
            "  Copied 10239 labels to /content/combined_dataset/train/labels\n",
            "\n",
            "Copying original valid data...\n",
            "  Copied 1018 images to /content/combined_dataset/valid/images\n",
            "  Copied 1018 labels to /content/combined_dataset/valid/labels\n",
            "\n",
            "Copying original test data...\n",
            "  Copied 495 images to /content/combined_dataset/test/images\n",
            "  Copied 495 labels to /content/combined_dataset/test/labels\n",
            "\n",
            "âœ… Original dataset files copied successfully to combined dataset!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73eecc50"
      },
      "source": [
        "## Copy Augmented Brightened Images and Labels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b432256",
        "outputId": "9ebfb305-0628-4099-9b5b-f95741eb3e3f"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Source paths for augmented images and original labels\n",
        "brightened_images_source_path = \"/content/dataset_bright/images\"\n",
        "original_labels_source_path = os.path.join(dataset.location, \"train\", \"labels\")\n",
        "\n",
        "# Destination paths in the combined dataset (train split)\n",
        "combined_train_images_dest_path = os.path.join(combined_dataset_base_path, \"train\", \"images\")\n",
        "combined_train_labels_dest_path = os.path.join(combined_dataset_base_path, \"train\", \"labels\")\n",
        "\n",
        "print(\"\\nCopying augmented brightened images and their original labels to combined dataset (train split)...\\n\")\n",
        "\n",
        "processed_images = 0\n",
        "processed_labels = 0\n",
        "\n",
        "# Copy brightened images and their corresponding labels\n",
        "if os.path.exists(brightened_images_source_path):\n",
        "    for image_file in os.listdir(brightened_images_source_path):\n",
        "        if image_file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.webp')):\n",
        "            source_image_path = os.path.join(brightened_images_source_path, image_file)\n",
        "            dest_image_path = os.path.join(combined_train_images_dest_path, \"bright_\" + image_file) # Add prefix to avoid name collision\n",
        "            shutil.copy(source_image_path, dest_image_path)\n",
        "            processed_images += 1\n",
        "\n",
        "            # Construct corresponding label file name\n",
        "            label_file = os.path.splitext(image_file)[0] + \".txt\"\n",
        "            source_label_path = os.path.join(original_labels_source_path, label_file)\n",
        "            dest_label_path = os.path.join(combined_train_labels_dest_path, \"bright_\" + label_file) # Add prefix\n",
        "\n",
        "            if os.path.exists(source_label_path):\n",
        "                shutil.copy(source_label_path, dest_label_path)\n",
        "                processed_labels += 1\n",
        "            else:\n",
        "                print(f\"Warning: Label file not found for {image_file} at {source_label_path}\")\n",
        "\n",
        "    print(f\"  Copied {processed_images} brightened images to {combined_train_images_dest_path}\")\n",
        "    print(f\"  Copied {processed_labels} corresponding labels to {combined_train_labels_dest_path}\")\n",
        "else:\n",
        "    print(f\"  Brightened images source path not found: {brightened_images_source_path}\")\n",
        "\n",
        "print(\"\\nâœ… Augmented brightened images and labels copied successfully!\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Copying augmented brightened images and their original labels to combined dataset (train split)...\n",
            "\n",
            "  Copied 10239 brightened images to /content/combined_dataset/train/images\n",
            "  Copied 10239 corresponding labels to /content/combined_dataset/train/labels\n",
            "\n",
            "âœ… Augmented brightened images and labels copied successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07e725bf"
      },
      "source": [
        "## Copy Augmented Motion-Blurred Images and Labels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "112f897e",
        "outputId": "5f544017-1ac6-4879-f772-b9a3e40eb589"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Source paths for augmented images and original labels\n",
        "motion_blurred_images_source_path = \"/content/dataset_motion_blur/images\"\n",
        "original_labels_source_path = os.path.join(dataset.location, \"train\", \"labels\")\n",
        "\n",
        "# Destination paths in the combined dataset (train split)\n",
        "combined_train_images_dest_path = os.path.join(combined_dataset_base_path, \"train\", \"images\")\n",
        "combined_train_labels_dest_path = os.path.join(combined_dataset_base_path, \"train\", \"labels\")\n",
        "\n",
        "print(\"\\nCopying augmented motion-blurred images and their original labels to combined dataset (train split)...\\n\")\n",
        "\n",
        "processed_images = 0\n",
        "processed_labels = 0\n",
        "\n",
        "# Copy motion-blurred images and their corresponding labels\n",
        "if os.path.exists(motion_blurred_images_source_path):\n",
        "    for image_file in os.listdir(motion_blurred_images_source_path):\n",
        "        if image_file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.webp')):\n",
        "            source_image_path = os.path.join(motion_blurred_images_source_path, image_file)\n",
        "            dest_image_path = os.path.join(combined_train_images_dest_path, \"motionblur_\" + image_file) # Add prefix to avoid name collision\n",
        "            shutil.copy(source_image_path, dest_image_path)\n",
        "            processed_images += 1\n",
        "\n",
        "            # Construct corresponding label file name\n",
        "            label_file = os.path.splitext(image_file)[0] + \".txt\"\n",
        "            source_label_path = os.path.join(original_labels_source_path, label_file)\n",
        "            dest_label_path = os.path.join(combined_train_labels_dest_path, \"motionblur_\" + label_file) # Add prefix\n",
        "\n",
        "            if os.path.exists(source_label_path):\n",
        "                shutil.copy(source_label_path, dest_label_path)\n",
        "                processed_labels += 1\n",
        "            else:\n",
        "                print(f\"Warning: Label file not found for {image_file} at {source_label_path}\")\n",
        "\n",
        "    print(f\"  Copied {processed_images} motion-blurred images to {combined_train_images_dest_path}\")\n",
        "    print(f\"  Copied {processed_labels} corresponding labels to {combined_train_labels_dest_path}\")\n",
        "else:\n",
        "    print(f\"  Motion-blurred images source path not found: {motion_blurred_images_source_path}\")\n",
        "\n",
        "print(\"\\nâœ… Augmented motion-blurred images and labels copied successfully!\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Copying augmented motion-blurred images and their original labels to combined dataset (train split)...\n",
            "\n",
            "  Copied 10239 motion-blurred images to /content/combined_dataset/train/images\n",
            "  Copied 10239 corresponding labels to /content/combined_dataset/train/labels\n",
            "\n",
            "âœ… Augmented motion-blurred images and labels copied successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a776afd"
      },
      "source": [
        "## Copy Augmented 1px Blurred Images and Labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08e495ea",
        "outputId": "bda95b3e-95c7-40d8-ba7f-dd326c4fbaeb"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Source paths for augmented images and original labels\n",
        "blurred_1px_images_source_path = \"/content/dataset_blur_1px/images\"\n",
        "original_labels_source_path = os.path.join(dataset.location, \"train\", \"labels\")\n",
        "\n",
        "# Destination paths in the combined dataset (train split)\n",
        "combined_train_images_dest_path = os.path.join(combined_dataset_base_path, \"train\", \"images\")\n",
        "combined_train_labels_dest_path = os.path.join(combined_dataset_base_path, \"train\", \"labels\")\n",
        "\n",
        "print(\"\\nCopying augmented 1px blurred images and their original labels to combined dataset (train split)...\\n\")\n",
        "\n",
        "processed_images = 0\n",
        "processed_labels = 0\n",
        "\n",
        "# Copy 1px blurred images and their corresponding labels\n",
        "if os.path.exists(blurred_1px_images_source_path):\n",
        "    for image_file in os.listdir(blurred_1px_images_source_path):\n",
        "        if image_file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.webp')):\n",
        "            source_image_path = os.path.join(blurred_1px_images_source_path, image_file)\n",
        "            dest_image_path = os.path.join(combined_train_images_dest_path, \"blur1px_\" + image_file) # Add prefix to avoid name collision\n",
        "            shutil.copy(source_image_path, dest_image_path)\n",
        "            processed_images += 1\n",
        "\n",
        "            # Construct corresponding label file name\n",
        "            label_file = os.path.splitext(image_file)[0] + \".txt\"\n",
        "            source_label_path = os.path.join(original_labels_source_path, label_file)\n",
        "            dest_label_path = os.path.join(combined_train_labels_dest_path, \"blur1px_\" + label_file) # Add prefix\n",
        "\n",
        "            if os.path.exists(source_label_path):\n",
        "                shutil.copy(source_label_path, dest_label_path)\n",
        "                processed_labels += 1\n",
        "            else:\n",
        "                print(f\"Warning: Label file not found for {image_file} at {source_label_path}\")\n",
        "\n",
        "    print(f\"  Copied {processed_images} 1px blurred images to {combined_train_images_dest_path}\")\n",
        "    print(f\"  Copied {processed_labels} corresponding labels to {combined_train_labels_dest_path}\")\n",
        "else:\n",
        "    print(f\"  1px blurred images source path not found: {blurred_1px_images_source_path}\")\n",
        "\n",
        "print(\"\\nâœ… Augmented 1px blurred images and labels copied successfully!\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Copying augmented 1px blurred images and their original labels to combined dataset (train split)...\n",
            "\n",
            "  Copied 10239 1px blurred images to /content/combined_dataset/train/images\n",
            "  Copied 10239 corresponding labels to /content/combined_dataset/train/labels\n",
            "\n",
            "âœ… Augmented 1px blurred images and labels copied successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e1935ab"
      },
      "source": [
        "## Generate New `data.yaml` File\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8718202f",
        "outputId": "3ad5412d-c8ab-4287-ab9d-103c3530712d"
      },
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "# Define the path for the new data.yaml file\n",
        "combined_dataset_path = \"/content/combined_dataset\"\n",
        "data_yaml_path = os.path.join(combined_dataset_path, \"data.yaml\")\n",
        "\n",
        "# Define relative paths for train, valid, and test splits\n",
        "train_path = os.path.join(combined_dataset_path, \"train\", \"images\")\n",
        "val_path = os.path.join(combined_dataset_path, \"valid\", \"images\")\n",
        "test_path = os.path.join(combined_dataset_path, \"test\", \"images\")\n",
        "\n",
        "# Try to get class names and number of classes from the original dataset's data.yaml\n",
        "original_data_yaml_path = os.path.join(dataset.location, \"data.yaml\")\n",
        "\n",
        "class_names = []\n",
        "num_classes = 0\n",
        "\n",
        "if os.path.exists(original_data_yaml_path):\n",
        "    with open(original_data_yaml_path, 'r') as f:\n",
        "        original_data = yaml.safe_load(f)\n",
        "    if 'names' in original_data:\n",
        "        class_names = original_data['names']\n",
        "        num_classes = len(class_names)\n",
        "    else:\n",
        "        print(\"Warning: 'names' key not found in original data.yaml. Using default class names.\")\n",
        "else:\n",
        "    print(\"Warning: Original data.yaml not found. Using default class names.\")\n",
        "\n",
        "# Fallback for class names if not found\n",
        "if not class_names:\n",
        "    # Assuming default classes if not found in original data.yaml or dataset object\n",
        "    class_names = ['class0', 'class1'] # Placeholder, adjust if specific names are known\n",
        "    num_classes = len(class_names)\n",
        "\n",
        "# Construct the content for the new data.yaml\n",
        "data_content = {\n",
        "    'path': combined_dataset_path, # Absolute path to the dataset root\n",
        "    'train': 'train/images',       # Relative path from 'path'\n",
        "    'val': 'valid/images',         # Relative path from 'path'\n",
        "    'test': 'test/images',         # Relative path from 'path'\n",
        "    'nc': num_classes,\n",
        "    'names': class_names\n",
        "}\n",
        "\n",
        "# Write the content to the new data.yaml file\n",
        "with open(data_yaml_path, 'w') as f:\n",
        "    yaml.dump(data_content, f, sort_keys=False)\n",
        "\n",
        "print(f\"âœ… New data.yaml created successfully at: {data_yaml_path}\")\n",
        "print(\"Content of new data.yaml:\")\n",
        "with open(data_yaml_path, 'r') as f:\n",
        "    print(f.read())\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… New data.yaml created successfully at: /content/combined_dataset/data.yaml\n",
            "Content of new data.yaml:\n",
            "path: /content/combined_dataset\n",
            "train: train/images\n",
            "val: valid/images\n",
            "test: test/images\n",
            "nc: 9\n",
            "names:\n",
            "- bicycle\n",
            "- bus\n",
            "- car\n",
            "- green_light\n",
            "- motobike\n",
            "- red_light\n",
            "- stop_line\n",
            "- truck\n",
            "- yellow_light\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b46be410"
      },
      "source": [
        "## Train YOLO Model with Combined Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1abfd14a",
        "outputId": "07f4f11a-f137-4a50-9c4d-e5591ce6f487"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the new data.yaml file\n",
        "combined_dataset_path = \"/content/combined_dataset\" # Re-defining for this cell's scope\n",
        "data_yaml_path = os.path.join(combined_dataset_path, \"data.yaml\")\n",
        "\n",
        "!yolo task=detect mode=train \\\n",
        "  model=yolov8n.pt \\\n",
        "  data={data_yaml_path} \\\n",
        "  epochs=10 \\\n",
        "  imgsz=416 \\\n",
        "  batch=16 \\\n",
        "  device=0 \\\n",
        "  cache=True"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.4.14 ğŸš€ Python-3.12.12 torch-2.9.0+cu128 CUDA:0 (Tesla T4, 14913MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/combined_dataset/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=9\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    753067  ultralytics.nn.modules.head.Detect           [9, 16, None, [64, 128, 256]] \n",
            "Model summary: 130 layers, 3,012,603 parameters, 3,012,587 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1229.9Â±1067.2 MB/s, size: 91.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/combined_dataset/train/labels.cache... 40956 images, 36 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 40956/40956 7.2Git/s 0.0s\n",
            "WARNING âš ï¸ \u001b[34m\u001b[1mtrain: \u001b[0m29.7GB RAM required to cache images with 50% safety margin but only 9.5/12.7GB available, not caching images\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1427.7Â±508.0 MB/s, size: 60.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/combined_dataset/valid/labels.cache... 1013 images, 3 backgrounds, 5 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1018/1018 62.8Mit/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/combined_dataset/valid/images/2392_jpg.rf.22b49f9a430e632fca9f01adf050fcd9.jpg: ignoring corrupt image/label: cannot identify image file '/content/combined_dataset/valid/images/2392_jpg.rf.22b49f9a430e632fca9f01adf050fcd9.jpg'\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/combined_dataset/valid/images/2394_jpg.rf.e88f7aa649ebf53fb7014be3af546ab0.jpg: ignoring corrupt image/label: cannot identify image file '/content/combined_dataset/valid/images/2394_jpg.rf.e88f7aa649ebf53fb7014be3af546ab0.jpg'\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/combined_dataset/valid/images/2418_jpg.rf.c95b0d96837d0df8ce64f2b2f099713b.jpg: ignoring corrupt image/label: cannot identify image file '/content/combined_dataset/valid/images/2418_jpg.rf.c95b0d96837d0df8ce64f2b2f099713b.jpg'\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/combined_dataset/valid/images/2443_jpg.rf.156b959b02e6492cf8fc9dbd24b7e99f.jpg: ignoring corrupt image/label: cannot identify image file '/content/combined_dataset/valid/images/2443_jpg.rf.156b959b02e6492cf8fc9dbd24b7e99f.jpg'\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/combined_dataset/valid/images/519_jpg.rf.a37995027954d24b24581ac22dec5a88.jpg: ignoring corrupt image/label: cannot identify image file '/content/combined_dataset/valid/images/519_jpg.rf.a37995027954d24b24581ac22dec5a88.jpg'\n",
            "WARNING âš ï¸ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.5GB RAM): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1013/1013 215.0it/s 4.7s\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000769, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Plotting labels to /content/runs/detect/train2/labels.jpg... \n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/train2\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/10     0.936G      1.141      1.412      1.047         22        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2560/2560 5.2it/s 8:08\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 32/32 5.1it/s 6.2s\n",
            "                   all       1013       3369      0.708      0.742      0.766      0.508\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/10      1.14G      1.055     0.8626      1.005         41        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2560/2560 5.5it/s 7:49\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 32/32 5.6it/s 5.7s\n",
            "                   all       1013       3369      0.768      0.742      0.813      0.549\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/10      1.14G      1.009     0.7654     0.9881         44        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2560/2560 5.5it/s 7:45\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 32/32 5.7it/s 5.6s\n",
            "                   all       1013       3369      0.728      0.806      0.826      0.572\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/10      1.15G     0.9559     0.6971     0.9666         29        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2560/2560 5.5it/s 7:43\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 32/32 5.7it/s 5.6s\n",
            "                   all       1013       3369      0.768      0.793      0.839      0.592\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/10      1.15G      0.904     0.6415     0.9469         39        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2560/2560 5.3it/s 7:59\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 32/32 7.0it/s 4.6s\n",
            "                   all       1013       3369      0.766      0.834      0.853      0.621\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/10      1.15G     0.8593     0.5977     0.9291         64        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2560/2560 5.5it/s 7:44\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 32/32 6.3it/s 5.1s\n",
            "                   all       1013       3369      0.771      0.831      0.853      0.615\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/10      1.15G     0.8218     0.5649     0.9138         48        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2560/2560 5.4it/s 7:50\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 32/32 6.9it/s 4.6s\n",
            "                   all       1013       3369      0.779      0.836      0.859      0.635\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/10      1.15G     0.7866     0.5376     0.9026         56        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2560/2560 5.5it/s 7:41\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 32/32 6.2it/s 5.1s\n",
            "                   all       1013       3369      0.781      0.836      0.862      0.637\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/10      1.16G     0.7586     0.5145     0.8924         22        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2560/2560 5.6it/s 7:38\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 32/32 5.1it/s 6.3s\n",
            "                   all       1013       3369       0.78       0.84      0.859      0.638\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/10      1.16G     0.7257     0.4909     0.8829         18        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2560/2560 5.4it/s 7:53\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 32/32 7.3it/s 4.4s\n",
            "                   all       1013       3369      0.773      0.867      0.862      0.643\n",
            "\n",
            "10 epochs completed in 1.318 hours.\n",
            "Optimizer stripped from /content/runs/detect/train2/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/runs/detect/train2/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/runs/detect/train2/weights/best.pt...\n",
            "Ultralytics 8.4.14 ğŸš€ Python-3.12.12 torch-2.9.0+cu128 CUDA:0 (Tesla T4, 14913MiB)\n",
            "Model summary (fused): 73 layers, 3,007,403 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 32/32 4.6it/s 6.9s\n",
            "                   all       1013       3369      0.773      0.868      0.862      0.643\n",
            "               bicycle        156        240      0.868       0.85      0.884      0.681\n",
            "                   bus        199        232       0.91      0.944      0.972      0.913\n",
            "                   car        272        613      0.756      0.821      0.874      0.693\n",
            "           green_light        146        284      0.617      0.841      0.798       0.44\n",
            "              motobike        276        784      0.599      0.755      0.719      0.449\n",
            "             red_light        211        390      0.608      0.933      0.787      0.436\n",
            "             stop_line        311        425      0.801      0.866      0.907      0.644\n",
            "                 truck        180        208      0.961      0.971      0.984      0.893\n",
            "          yellow_light        167        193      0.838      0.829      0.835      0.637\n",
            "Speed: 0.1ms preprocess, 1.1ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/train2\u001b[0m\n",
            "ğŸ’¡ Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Evaluation"
      ],
      "metadata": {
        "id": "D5uvDb-sIFuo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10afb4fb",
        "outputId": "d81acdd2-ef17-49e1-b1c7-4173f6e5519e"
      },
      "source": [
        "from ultralytics import YOLO\n",
        "import os\n",
        "\n",
        "# Define the path to the combined dataset's data.yaml\n",
        "combined_dataset_path = \"/content/combined_dataset\"\n",
        "data_yaml_path = os.path.join(combined_dataset_path, \"data.yaml\")\n",
        "\n",
        "# Load the custom trained model (replace 'train2' with your actual run directory if different)\n",
        "# The last training run produced results in '/content/runs/detect/train2/weights/best.pt'\n",
        "model_path = '/content/runs/detect/train2/weights/best.pt'\n",
        "\n",
        "# Check if the model exists before loading\n",
        "if os.path.exists(model_path):\n",
        "    model = YOLO(model_path)\n",
        "    print(f\"âœ… Model loaded successfully from: {model_path}\")\n",
        "else:\n",
        "    print(f\"âŒ Model not found at: {model_path}. Please ensure training completed successfully and check the path.\")\n",
        "    # You might want to exit or handle this error more gracefully\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model loaded successfully from: /content/runs/detect/train2/weights/best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Confusion Metics and Accuracy Score"
      ],
      "metadata": {
        "id": "GMuS0NrQKvQ6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92d3a882",
        "outputId": "3f798b3d-3a9d-4257-fb55-6340e5ca9834"
      },
      "source": [
        "# Run validation on the test dataset to get metrics and confusion matrix\n",
        "# This will save results, including plots and a confusion matrix, in a new 'predict' directory within the current run folder.\n",
        "\n",
        "# The 'val' directory might be 'val2' if there were previous validation runs.\n",
        "metrics = model.val(data=data_yaml_path, split='test', imgsz=416, batch=16, save_json=True, plots=True)\n",
        "\n",
        "# Print the metrics\n",
        "print(\"\\nâœ… Validation Metrics:\")\n",
        "print(f\"mAP50-95: {metrics.box.map}\")\n",
        "print(f\"mAP50: {metrics.box.map50}\")\n",
        "print(f\"mAP75: {metrics.box.map75}\")\n",
        "\n",
        "# The confusion matrix and other plots are saved in the 'runs/detect/valX' directory (e.g., 'runs/detect/val2')\n",
        "print(\"\\nğŸ“„ Confusion matrix and other plots are saved in the run directory (e.g., '/content/runs/detect/val').\")\n",
        "print(\"Look for files like 'confusion_matrix.png', 'results.png', etc.\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.4.14 ğŸš€ Python-3.12.12 torch-2.9.0+cu128 CUDA:0 (Tesla T4, 14913MiB)\n",
            "Model summary (fused): 73 layers, 3,007,403 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 17.7Â±8.7 MB/s, size: 48.7 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/combined_dataset/test/labels... 495 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 495/495 534.6it/s 0.9s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/combined_dataset/test/labels.cache\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 5.8it/s 5.4s\n",
            "                   all        495       2210      0.768      0.827      0.845      0.622\n",
            "               bicycle         32         70      0.759      0.729      0.668      0.504\n",
            "                   bus         53         66      0.815      0.879      0.935      0.822\n",
            "                   car        223        486      0.806      0.809      0.898      0.713\n",
            "           green_light        115        199      0.637      0.811      0.789      0.468\n",
            "              motobike        223        617      0.676      0.747      0.779      0.496\n",
            "             red_light        164        299      0.624      0.799      0.794      0.421\n",
            "             stop_line        230        292      0.837      0.877       0.91      0.645\n",
            "                 truck         45         48      0.913      0.958      0.982      0.867\n",
            "          yellow_light        121        133      0.847      0.833      0.851      0.662\n",
            "Speed: 0.6ms preprocess, 2.4ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
            "Saving /content/runs/detect/val/predictions.json...\n",
            "Results saved to \u001b[1m/content/runs/detect/val\u001b[0m\n",
            "\n",
            "âœ… Validation Metrics:\n",
            "mAP50-95: 0.6219839002710466\n",
            "mAP50: 0.8450568848564388\n",
            "mAP75: 0.6624156533271209\n",
            "\n",
            "ğŸ“„ Confusion matrix and other plots are saved in the run directory (e.g., '/content/runs/detect/val').\n",
            "Look for files like 'confusion_matrix.png', 'results.png', etc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Define the path to the newly trained model from the 'train2' run\n",
        "model_source_path = \"runs/detect/train2/weights/best.pt\"\n",
        "model_destination_name = \"yolo1v8s.pt\"\n",
        "\n",
        "if os.path.exists(model_source_path):\n",
        "  # If the file hasn't been renamed already, rename it\n",
        "  if not os.path.exists(model_destination_name):\n",
        "    os.rename(model_source_path, model_destination_name)\n",
        "    print(f\"Renamed {model_source_path} to {model_destination_name}\")\n",
        "  else:\n",
        "    print(f\"Model already renamed and available as {model_destination_name}\")\n",
        "\n",
        "  # Download the model\n",
        "  files.download(model_destination_name)\n",
        "else:\n",
        "  print(f\"Model not found at: {model_source_path}. Please ensure the training completed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "XO7XLgKLP0EN",
        "outputId": "683d3d4e-9ede-472d-8ce7-4c3fb95fd114"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed runs/detect/train2/weights/best.pt to yolo1v8s.pt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d2c1c279-c09f-4c6d-b1a3-f62e54f935f0\", \"yolo1v8s.pt\", 6216490)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/SidharthMN/CDLAB/blob/main/roboflow_data_on_remote_train.ipynb",
      "authorship_tag": "ABX9TyMVxh5JI41i4ZeG204O4r7B",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}